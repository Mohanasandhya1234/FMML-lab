{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohanasandhya1234/FMML-lab/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef9aca6-d4bb-45b5-8111-12c2b7fe4d50"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03e27872-b457-4bab-c38f-d01d324b7b7e"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ed068f-4972-43c3-f346-35cdf2c9b141"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5604ac72-1e8e-454a-ac28-dbd0dc3676b7"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "360b57a6-8b74-4f0a-fd7b-22c679708a39"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aad3d2d7-42cd-4136-dac2-f8f1d59e5d73"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6919a911-e860-4cb8-d3cc-78c3328cdc94"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "answers\n",
        "\n",
        "1.\n",
        "Averaging validation accuracy across multiple splits, often referred to as k-fold cross-validation, can indeed provide more consistent and reliable results compared to a single train-test split. This approach helps in reducing the variance in your evaluation and can give you a more robust estimate of your model's performance. Here's why it is beneficial:\n",
        "\n",
        "Reduced Variance: In a single train-test split, your model's performance can be highly dependent on the specific data points that end up in the training and testing sets. This can lead to high variance in your performance estimate. By using k-fold cross-validation, you average the results over multiple splits, which reduces the impact of random data splitting and provides a more stable evaluation of your model.\n",
        "\n",
        "Better Generalization: Cross-validation helps you assess how well your model generalizes to unseen data. Averaging performance across multiple folds provides a more accurate estimate of your model's expected performance on new, unseen data.\n",
        "\n",
        "Utilizes All Data: In k-fold cross-validation, you use all available data for both training and validation, ensuring that every data point contributes to the evaluation of the model's performance. This is especially important when you have a limited dataset.\n",
        "\n",
        "Identifies Overfitting: Cross-validation can help you identify whether your model is overfitting. If you see a significant discrepancy between training and validation performance across folds, it may indicate overfitting, which can guide you in adjusting your model or regularization techniques.\n",
        "\n",
        "Robustness to Data Distribution: Cross-validation can provide a more reliable estimate of performance when dealing with imbalanced datasets or datasets with varying data distributions across different classes or categories.\n",
        "\n",
        "However, it's important to keep in mind a few considerations:\n",
        "\n",
        "Cross-validation can be computationally expensive, as you're training and evaluating your model multiple times (k times, where k is the number of folds). This may not be practical for very large datasets or complex models.\n",
        "\n",
        "The choice of k (the number of folds) can affect the results. Common values for k include 5 or 10, but the optimal choice may depend on the size and nature of your dataset.\n",
        "\n",
        "Ensure that you maintain consistent data preprocessing and feature engineering across all folds to ensure a fair comparison.\n",
        "\n",
        "In summary, averaging validation accuracy across multiple splits using k-fold cross-validation is a valuable technique for obtaining more consistent and reliable model evaluation results. It helps mitigate the impact of random data splitting and provides a better assessment of your model's generalization performance.\n",
        "\n",
        "\n",
        "2.\n",
        "K-fold cross-validation provides a more accurate estimate of how well your model is likely to perform on unseen data compared to a single train-test split. However, it's important to note that the test accuracy estimated through cross-validation is still an estimate, not an exact measure of your model's performance on a completely independent test dataset. Here's why k-fold cross-validation is beneficial:\n",
        "\n",
        "Reduced Variance: By averaging the performance over multiple folds, cross-validation provides a more stable and less variable estimate of your model's performance. This is especially important when you have limited data, and a single random train-test split can lead to widely varying results.\n",
        "\n",
        "Better Generalization Assessment: Cross-validation gives you a better understanding of how well your model generalizes to unseen data because it evaluates the model on multiple different subsets of your dataset. This can help you identify if your model is overfitting or underfitting.\n",
        "\n",
        "Utilizes All Data: Cross-validation ensures that all data points in your dataset are used for both training and validation across the multiple folds. This makes the evaluation more representative of your overall data.\n",
        "\n",
        "Statistical Significance: Averaging results over multiple folds provides a more statistically significant estimate of your model's performance. This is particularly important when you want to make inferences about your model's performance with a certain level of confidence.\n",
        "\n",
        "While k-fold cross-validation provides a more accurate estimate of model performance compared to a single train-test split, it's essential to keep in mind that the estimate is still based on your existing dataset. It does not replace the need for a truly independent test dataset, which is often used for final model evaluation.\n",
        "\n",
        "In practice, it's common to perform k-fold cross-validation to tune hyperparameters, assess model performance, and get an initial sense of how well the model is likely to perform on new, unseen data. After you've iteratively refined your model using cross-validation, you can then use a separate, completely independent test dataset to get a final, unbiased assessment of your model's performance before deploying it in a real-world application.\n",
        "\n",
        "\n",
        "3.\n",
        "The number of iterations in k-fold cross-validation can impact the reliability and precision of the estimate of your model's performance. In general, increasing the number of iterations (folds) can provide a more accurate and stable estimate, but it comes with trade-offs in terms of computational cost.\n",
        "\n",
        "Here's how the number of iterations can affect the estimate:\n",
        "\n",
        "More Iterations (Folds):\n",
        "\n",
        "Advantages:\n",
        "Reduces the impact of random data splitting: With more folds, the influence of the initial random data partitioning is reduced because you're averaging results over multiple splits.\n",
        "Provides a more stable estimate: Averaging performance over more iterations tends to yield a more stable estimate of your model's performance, which can be particularly beneficial when dealing with small or noisy datasets.\n",
        "Better assessment of variability: More iterations can help you understand the variability in your model's performance across different subsets of the data.\n",
        "Disadvantages:\n",
        "Increased computational cost: Running cross-validation with a higher number of folds requires training and evaluating the model more times, which can be computationally expensive, especially for large datasets or complex models.\n",
        "Smaller training sets: As you increase the number of folds, the training sets in each fold become smaller, which may limit the ability of your model to capture complex patterns in the data.\n",
        "Fewer Iterations (Folds):\n",
        "\n",
        "Advantages:\n",
        "Lower computational cost: Using fewer folds is computationally less demanding, making it more practical for large datasets and resource-constrained environments.\n",
        "Larger training sets: With fewer folds, each training set is larger, which can be advantageous when you have limited data.\n",
        "Disadvantages:\n",
        "Greater impact of random data splitting: A single random data split can have a more substantial influence on the results, leading to higher variability in your performance estimate.\n",
        "Less stable estimate: With fewer iterations, the estimate of your model's performance may be less stable and more dependent on the particular data split.\n",
        "In practice, the choice of the number of iterations (folds) in k-fold cross-validation is often a trade-off between computational cost and the quality of the performance estimate. Common values for k include 5 and 10, but the optimal choice may depend on factors like the size of your dataset, the complexity of your model, and available computational resources.\n",
        "\n",
        "It's a good practice to start with a reasonable number of folds and then assess whether increasing or decreasing the number of folds has a significant impact on your model evaluation results.\n",
        "\n",
        "\n",
        "4.\n",
        "Increasing the number of iterations (folds) in k-fold cross-validation can help mitigate some of the challenges associated with very small training datasets or validation datasets to a certain extent, but it doesn't fundamentally solve the problem of limited data. It can provide more robust estimates of model performance, but there are limitations to what increased iterations can achieve in such scenarios.\n",
        "\n",
        "Here are some considerations:\n",
        "\n",
        "Advantages of Increasing Iterations (Folds):\n",
        "\n",
        "Reduced Impact of Randomness: With more iterations, you reduce the impact of the initial random data splitting. This means you'll have a better sense of how well your model performs across different subsets of the data.\n",
        "\n",
        "More Stable Estimates: Averaging results over multiple folds tends to yield a more stable estimate of your model's performance, which can be helpful when dealing with small datasets. It can provide a more reliable assessment of your model's generalization ability.\n",
        "\n",
        "Limitations and Considerations:\n",
        "\n",
        "Limited Data: If your training dataset is very small, increasing the number of iterations doesn't magically create more training data. Each fold will still have a small training set, which may limit your model's ability to capture complex patterns.\n",
        "\n",
        "Increased Computational Cost: Running cross-validation with a high number of folds can be computationally expensive, especially if your dataset is small, as it means training and evaluating the model multiple times. This might not be practical in resource-constrained environments.\n",
        "\n",
        "Overfitting Risk: With very small training sets in each fold, there's a higher risk of overfitting. Models trained on small datasets may perform well on those specific subsets but may not generalize well to new, unseen data.\n",
        "\n",
        "Biased Estimates: If the dataset is extremely small, even with increased iterations, you might still end up with biased estimates due to the limited diversity in the data.\n",
        "\n",
        "In cases of extremely small datasets, you should carefully consider the trade-offs between using k-fold cross-validation with more iterations and alternative approaches such as:\n",
        "\n",
        "Holdout Validation: If you have very limited data, you might consider a simple holdout validation where you split your dataset into a larger portion for training and a smaller portion for validation. While this doesn't provide multiple estimates like k-fold cross-validation, it can be less computationally intensive.\n",
        "\n",
        "Data Augmentation: If possible, you can apply data augmentation techniques to artificially increase the effective size of your training dataset by generating new training examples from existing ones.\n",
        "\n",
        "Regularization: Use regularization techniques like dropout or L1/L2 regularization to help prevent overfitting when dealing with small datasets.\n",
        "\n",
        "In summary, increasing the number of iterations in cross-validation can improve the robustness of your performance estimates, but it doesn't solve the fundamental issue of a very small dataset. You should carefully consider your options and potentially explore techniques beyond cross-validation when dealing with extremely limited data.\n"
      ],
      "metadata": {
        "id": "lmjKd7wc6e1D"
      }
    }
  ]
}